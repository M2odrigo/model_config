# Configuraciones de la red 
#   Se declaran todos los atributos para construir la red con el api de KERAS
#   El backend utilizado es corresponde a TensorFlow de GOOGLE
#   Las opciones que se pueden pasar a la red se encuentran detalladas en:
#   https://keras.io/

[ints]
# Cantidad de parametros que recibira la primera hidden
cant_input = 60
# Cantidad de capas, incluyendo el output layer -- hidden+hidden+output => cant_capas = 3
cant_capas = 3
# Cantidad de neuronas por cada capa, incluye el output => 60,30,1
cant_neuronas = 60,30,1
# Cantidad de datos por lote
batch_size = 100
# Cantidad de epochs
cant_epochs = 100
# indica el intervalo de los epochs, debe ser divisible entre la cant_epochs
    #100 = se almacenaran los weights para los siguientes epcohs 10,20,30,40,50,60,70,80,90,100
intervalo = 10
# Cantidad de veces a ejecutar la misma configuracion de la red 
cant_ejecucion = 2

[strings]
# Activation : 'sigmoid', 'relu', se debe declarar uno por cada capa, incluido el output layer
# Optimizer - metodo de aprendizaje, solo uno para toda la red.
# Loss - 
activation = relu,relu,sigmoid
optimizer = adam
loss = binary_crossentropy

#dropout
#dropout_value, definir desde 0.2 hasta 0.5 como max, 0 para no agregar dropout, respeta el orden de capas. --> 0.2,0.5,0.2
[dropout]
dropout_value=0,0,0

[archives]
#modo de entrenamiento train -- train-test
mode = train
dataset = data/sonar.csv
dataset_train = data/sonar-train.csv
dataset_test = data/sonar-test.csv

[regularizers]
##Los penalties para la regularizacion pueden ser:
    ###keras.regularizers.l1(0.)
    ###keras.regularizers.l2(0.)
    ###keras.regularizers.l1_l2(0.)
#habilitar true/false el kernel_regularizer
kernel_regularizer=false
#elegir el tipo valor/modo de regularizacion 
kernel_value=l2
kernel_l_value=0.01

bias_regularizer=true
bias_value=l1
bias_l_value=0.

activity_regularizer=false
activity_value=l1_l2
activity_l_value=0.

[perceptron]
# Cantidad de epochs para el perceptron
cant_epochs = 100
# Learning rate del perceptron
eta = 1
