# Configuraciones de la red 
#   Se declaran todos los atributos para construir la red con el api de KERAS
#   El backend utilizado es corresponde a TensorFlow de GOOGLE
#   Las opciones que se pueden pasar a la red se encuentran detalladas en:
#   https://keras.io/

[ints]
# Cantidad de parametros que recibira la primera hidden
cant_input = 60
# Cantidad de capas, incluyendo el output layer -- hidden+hidden+output => cant_capas = 3
cant_capas = 3
# Cantidad de neuronas por cada capa, incluye el output => 60,30,1
cant_neuronas = 60,30,1
# Cantidad de datos por lote
batch_size = 100
# Cantidad de epochs
cant_epochs = 10

[strings]
# Activation : 'sigmoid', 'relu', se debe declarar uno por cada capa, incluido el output layer
# Optimizer - metodo de aprendizaje, solo uno para toda la red.
# Loss - 
activation = relu,relu,sigmoid
optimizer = adam
loss = binary_crossentropy

#dropout
#droput_capa: definir la(s) capa(s) donde ira el dropout, emepieza con indice ZERO --> 0,1,2
#dropout_value, definir desde 0.2 hasta 0.5 como max, respeta el orden de capas. --> 0.2,0.5,0.2
[floats]
droput_capa = 0,1,2
dropout_value = 0

[archives]
#modo de entrenamiento train -- train-test
mode = train
dataset = data/sonar.csv
dataset_train = data/sonar-train.csv
dataset_test = data/sonar-test.csv

[regularizers]
##Los penalties para la regularizacion pueden ser:
    ###keras.regularizers.l1(0.)
    ###keras.regularizers.l2(0.)
    ###keras.regularizers.l1_l2(0.)
#habilitar true/false el kernel_regularizer
kernel_regularizer=true
#elegir el tipo valor/modo de regularizacion 
kernel_value=l2
kernel_l_value=0.01

bias_regularizer=false
bias_value=l1
bias_l_value=0.

activity_regularizer=false
activity_value=l1_l2
activity_l_value=0.

[perceptron]
# Cantidad de epochs para el perceptron
cant_epochs = 100
# Learning rate del perceptron
eta = 1
